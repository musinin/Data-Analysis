import streamlit as st
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from konlpy.tag import Okt
import pickle

st.title('--Uber pickups in NYC')

DATE_COLUMN = 'date/time'
DATA_URL = ('https://s3-us-west-2.amazonaws.com/'
            'streamlit-demo-data/uber-raw-data-sep14.csv.gz')

@st.cache
def load_data(nrows):
    data = pd.read_csv(DATA_URL, nrows=nrows)
    lowercase = lambda x: str(x).lower()
    data.rename(lowercase, axis='columns', inplace=True)
    data[DATE_COLUMN] = pd.to_datetime(data[DATE_COLUMN])
    return data

data_load_state = st.text('Loading data...')
data = load_data(10000)
data_load_state.text("Done! (using st.cache)")

if st.button("ë²„íŠ¼ í´ë¦­"):
    st.write("Data Loading..")
    # ë°ì´í„° ë¡œë”© í•¨ìˆ˜ëŠ” ì—¬ê¸°ì—!

checkbox_btn1 = st.checkbox('ì˜µì…˜1')
checkbox_btn2 = st.checkbox('ì˜µì…˜2')

if checkbox_btn1:
    st.write('Great!')

option = st.selectbox('Please select in selectbox!',
                    ('kyle', 'seongyun', 'zzsza'))

st.write('You selected:', option)

if st.checkbox('Show raw data'):
    st.subheader('Raw data')
    st.write(data)

st.subheader('Number of pickups by hour')
hist_values = np.histogram(data[DATE_COLUMN].dt.hour, bins=24, range=(0,24))[0]
st.bar_chart(hist_values)

hour_to_filter = st.slider('hour', 0, 23, 17)
filtered_data = data[data[DATE_COLUMN].dt.hour == hour_to_filter]

st.subheader('Map of all pickups at %s:00' % hour_to_filter)
st.map(filtered_data)

values = st.text_input("ëŒ€ì„  í›„ë³´ ì…ë ¥")
st.write('input value:', values)



# 2. ì§‘ê°’ ì˜ˆì¸¡ ëª¨ë¸

with open("xgboost_feature_full.pkl", "rb") as f:
    model = pickle.load(f)

st.title("XGBoost Prediction App")

# ì…ë ¥ ë°›ê¸° ì˜ˆì‹œ
age = st.number_input("Enter Age")
income = st.number_input("Enter Income")

# ì˜ˆì¸¡
if st.button("Predict"):
    input_df = pd.DataFrame([[age, income]], columns=["age", "income"])
    pred = model.predict(input_df)
    st.write("Prediction:", pred[0])

# 3. ê°ì„±ë¡œë“œ
# ì‚¬ì „ ì •ì˜
index_to_tag = {0: 'ë¶€ì •', 1: 'ê¸ì •'}
word_to_index = torch.load('word_to_index.pth')  # ì˜ˆ: {'ì¢‹ë‹¤': 5, 'ì‹«ë‹¤': 7, ...}
stopwords = ['ë„', 'ëŠ”', 'ë‹¤', 'ì˜', 'ê°€', 'ì´', 'ì€', 'í•œ', 'ì—', 'í•˜', 'ê³ ',
             'ì„', 'ë¥¼', 'ì¸', 'ë“¯', 'ê³¼', 'ì™€', 'ë„¤', 'ë“¤', 'ë“¯', 'ì§€', 'ì„', 'ê²Œ']
device = torch.device("cpu")
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
okt = Okt()


# ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (LSTM êµ¬ì¡°ì— ë§ê²Œ ë°˜ë“œì‹œ ìˆ˜ì •)
class SentimentLSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super(SentimentLSTM, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = self.embedding(x)
        _, (hidden, _) = self.lstm(x)
        output = self.fc(hidden[-1])
        return output

# ìºì‹œëœ ëª¨ë¸ ë¡œë“œ
@st.cache_resource
def load_model():
    vocab_size = 17403  # vocab_size ìˆ˜ì • (ì²´í¬í¬ì¸íŠ¸ì™€ ì¼ì¹˜í•˜ë„ë¡)
    embedding_dim = 100
    hidden_dim = 128
    output_dim = 2
    model = SentimentLSTM(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)
    model.load_state_dict(torch.load('best_model_checkpoint.pth', map_location=device), strict=False)
    model.eval()
    return model

# ëª¨ë¸ ë¡œë“œ
model = load_model()

# ì²´í¬í¬ì¸íŠ¸ì™€ ì¼ì¹˜í•˜ëŠ” vocab_sizeë¡œ ëª¨ë¸ ë¡œë“œ
vocab_size = 17403  # ì²´í¬í¬ì¸íŠ¸ì™€ ì¼ì¹˜í•˜ë„ë¡ ìˆ˜ì •
embedding_dim = 100
hidden_dim = 128
output_dim = 2

model = SentimentLSTM(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)
model.load_state_dict(torch.load('best_model_checkpoint.pth', map_location=device), strict=False)
model.eval()

# ì˜ˆì¸¡ í•¨ìˆ˜
def predict(text):
    tokens = okt.morphs(text, stem=True)
    tokens = [word for word in tokens if word not in stopwords]
    token_indices = [word_to_index.get(word, 1) for word in tokens]
    input_tensor = torch.tensor([token_indices], dtype=torch.long).to(device)

    with torch.no_grad():
        logits = model(input_tensor)
        predicted_index = torch.argmax(logits, dim=1).item()
        return index_to_tag[predicted_index]

# Streamlit UI
st.title("ê°ì„± ë¶„ì„ (LSTM ê¸°ë°˜)")

user_input = st.text_input("ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”:")

if st.button("ì˜ˆì¸¡"):
    if user_input:
        result = predict(user_input)
        st.write(f"ğŸ‘‰ ì˜ˆì¸¡ ê²°ê³¼: **{result}**")
    else:
        st.warning("ë¬¸ì¥ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")